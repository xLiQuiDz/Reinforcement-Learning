{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch as T\n",
    "import gym\n",
    "\n",
    "import collections \n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatActionAndMaxFrame(gym.Wrapper):\n",
    "    def __init__(self, env=None, repeat=4, clip_reward=False, no_ops=0,\n",
    "                 fire_first=False):\n",
    "        super(RepeatActionAndMaxFrame, self).__init__(env)\n",
    "        self.repeat = repeat\n",
    "        self.shape = env.observation_space.low.shape\n",
    "        self.frame_buffer = np.zeros_like((2, self.shape))\n",
    "        self.clip_reward = clip_reward\n",
    "        self.no_ops = no_ops\n",
    "        self.fire_first = fire_first\n",
    "\n",
    "    def step(self, action):\n",
    "        t_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self.repeat):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if self.clip_reward:\n",
    "                reward = np.clip(np.array([reward]), -1, 1)[0]\n",
    "            t_reward += reward\n",
    "            idx = i % 2\n",
    "            self.frame_buffer[idx] = obs\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])\n",
    "        return max_frame, t_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        no_ops = np.random.randint(self.no_ops)+1 if self.no_ops > 0 else 0\n",
    "        for _ in range(no_ops):\n",
    "            _, _, done, _ = self.env.step(0)\n",
    "            if done:\n",
    "                self.env.reset()\n",
    "        if self.fire_first:\n",
    "            assert self.env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "            obs, _, _, _ = self.env.step(1)\n",
    "\n",
    "        self.frame_buffer = np.zeros_like((2,self.shape))\n",
    "        self.frame_buffer[0] = obs\n",
    "\n",
    "        return obs\n",
    "\n",
    "class PreprocessFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, shape, env=None):\n",
    "        super(PreprocessFrame, self).__init__(env)\n",
    "        self.shape = (shape[2], shape[0], shape[1])\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,\n",
    "                                    shape=self.shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "        resized_screen = cv2.resize(new_frame, self.shape[1:],\n",
    "                                    interpolation=cv2.INTER_AREA)\n",
    "        new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)\n",
    "        new_obs = new_obs / 255.0\n",
    "\n",
    "        return new_obs\n",
    "\n",
    "# \n",
    "class StackFrames(gym.ObservationWrapper):\n",
    "    def __init__(self, env, repeat):\n",
    "        super(StackFrames, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "                            env.observation_space.low.repeat(repeat, axis=0),\n",
    "                            env.observation_space.high.repeat(repeat, axis=0),\n",
    "                            dtype=np.float32)\n",
    "        self.stack = collections.deque(maxlen=repeat)\n",
    "\n",
    "    def reset(self):\n",
    "        self.stack.clear()\n",
    "        observation = self.env.reset()\n",
    "        for _ in range(self.stack.maxlen):\n",
    "            self.stack.append(observation)\n",
    "\n",
    "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.stack.append(observation)\n",
    "\n",
    "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
    "\n",
    "def make_env(env_name, shape=(84,84,1), repeat=4, clip_rewards=False,\n",
    "             no_ops=0, fire_first=False):\n",
    "    env = gym.make(env_name)\n",
    "    env = RepeatActionAndMaxFrame(env, repeat, clip_rewards, no_ops, fire_first)\n",
    "    env = PreprocessFrame(shape, env)\n",
    "    env = StackFrames(env, repeat)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
    "                                     dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
    "                                         dtype=np.float32)\n",
    "\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "\n",
    "    def store_experience(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, n_actions, name, input_dims, chkpt_dir):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_dims[0], 32, 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n",
    "\n",
    "        fc_input_dims = self.calculate_conv_output_dims(input_dims)\n",
    "\n",
    "        self.fc1 = nn.Linear(fc_input_dims, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def calculate_conv_output_dims(self, input_dims):\n",
    "        state = T.zeros(1, *input_dims)\n",
    "        dims = self.conv1(state)\n",
    "        dims = self.conv2(dims)\n",
    "        dims = self.conv3(dims)\n",
    "        return int(np.prod(dims.size()))\n",
    "\n",
    "    def forward(self, state):\n",
    "        conv1 = F.relu(self.conv1(state))\n",
    "        conv2 = F.relu(self.conv2(conv1))\n",
    "        conv3 = F.relu(self.conv3(conv2))\n",
    "        # conv3 shape is BS x n_filters x H x W\n",
    "        conv_state = conv3.view(conv3.size()[0], -1)\n",
    "        # conv_state shape is BS x (n_filters * H * W)\n",
    "        flat1 = F.relu(self.fc1(conv_state))\n",
    "        actions = self.fc2(flat1)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, gamma, epsilon, lr, n_actions, input_dims,\n",
    "                 mem_size, batch_size, eps_min=0.01, eps_dec=5e-7,\n",
    "                 replace=1000, algo=None, env_name=None, chkpt_dir='tmp/dqn'):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.n_actions = n_actions\n",
    "        self.input_dims = input_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_dec = eps_dec\n",
    "        self.replace_target_cnt = replace\n",
    "        self.algo = algo\n",
    "        self.env_name = env_name\n",
    "        self.chkpt_dir = chkpt_dir\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        self.memory = ReplayBuffer(mem_size, input_dims, n_actions)\n",
    "\n",
    "        self.q_eval = DeepQNetwork(self.lr, self.n_actions,\n",
    "                                    input_dims=self.input_dims,\n",
    "                                    name=self.env_name+'_'+self.algo+'_q_eval',\n",
    "                                    chkpt_dir=self.chkpt_dir)\n",
    "\n",
    "        self.q_next = DeepQNetwork(self.lr, self.n_actions,\n",
    "                                    input_dims=self.input_dims,\n",
    "                                    name=self.env_name+'_'+self.algo+'_q_next',\n",
    "                                    chkpt_dir=self.chkpt_dir)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation],dtype=T.float).to(self.q_eval.device)\n",
    "            actions = self.q_eval.forward(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_experience(self, state, action, reward, state_, done):\n",
    "        self.memory.store_experience(state, action, reward, state_, done)\n",
    "\n",
    "    def sample_memory(self):\n",
    "        state, action, reward, new_state, done = \\\n",
    "                                self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        states = T.tensor(state).to(self.q_eval.device)\n",
    "        rewards = T.tensor(reward).to(self.q_eval.device)\n",
    "        dones = T.tensor(done).to(self.q_eval.device)\n",
    "        actions = T.tensor(action).to(self.q_eval.device)\n",
    "        states_ = T.tensor(new_state).to(self.q_eval.device)\n",
    "\n",
    "        return states, actions, rewards, states_, dones\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        if self.learn_step_counter % self.replace_target_cnt == 0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "\n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "                           if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    def save_models(self):\n",
    "        self.q_eval.save_checkpoint()\n",
    "        self.q_next.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.q_eval.load_checkpoint()\n",
    "        self.q_next.load_checkpoint()\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.q_eval.optimizer.zero_grad()\n",
    "\n",
    "        self.replace_target_network()\n",
    "\n",
    "        states, actions, rewards, states_, dones = self.sample_memory()\n",
    "        indices = np.arange(self.batch_size)\n",
    "\n",
    "        q_pred = self.q_eval.forward(states)[indices, actions]\n",
    "        q_next = self.q_next.forward(states_).max(dim=1)[0]\n",
    "\n",
    "        q_next[dones] = 0.0\n",
    "        q_target = rewards + self.gamma*q_next\n",
    "\n",
    "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n",
    "        loss.backward()\n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        self.decrement_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 score:  -21.0  average score -21.0 best score -inf epsilon 0.99 steps 974\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  1 score:  -21.0  average score -21.0 best score -21.00 epsilon 0.98 steps 1876\n",
      "episode:  2 score:  -21.0  average score -21.0 best score -21.00 epsilon 0.97 steps 2747\n",
      "episode:  3 score:  -21.0  average score -21.0 best score -21.00 epsilon 0.96 steps 3558\n",
      "episode:  4 score:  -21.0  average score -21.0 best score -21.00 epsilon 0.96 steps 4322\n",
      "episode:  5 score:  -21.0  average score -21.0 best score -21.00 epsilon 0.95 steps 5104\n",
      "episode:  6 score:  -20.0  average score -20.9 best score -21.00 epsilon 0.94 steps 6026\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  7 score:  -21.0  average score -20.9 best score -20.86 epsilon 0.93 steps 6852\n",
      "episode:  8 score:  -20.0  average score -20.8 best score -20.86 epsilon 0.92 steps 7871\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  9 score:  -21.0  average score -20.8 best score -20.78 epsilon 0.91 steps 8779\n",
      "episode:  10 score:  -21.0  average score -20.8 best score -20.78 epsilon 0.90 steps 9603\n",
      "episode:  11 score:  -21.0  average score -20.8 best score -20.78 epsilon 0.90 steps 10414\n",
      "episode:  12 score:  -21.0  average score -20.8 best score -20.78 epsilon 0.89 steps 11239\n",
      "episode:  13 score:  -21.0  average score -20.9 best score -20.78 epsilon 0.88 steps 12003\n",
      "episode:  14 score:  -20.0  average score -20.8 best score -20.78 epsilon 0.87 steps 12881\n",
      "episode:  15 score:  -19.0  average score -20.7 best score -20.78 epsilon 0.86 steps 14020\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  16 score:  -20.0  average score -20.6 best score -20.69 epsilon 0.85 steps 15107\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  17 score:  -21.0  average score -20.7 best score -20.65 epsilon 0.84 steps 15957\n",
      "episode:  18 score:  -21.0  average score -20.7 best score -20.65 epsilon 0.83 steps 17008\n",
      "episode:  19 score:  -20.0  average score -20.6 best score -20.65 epsilon 0.82 steps 17966\n",
      "episode:  20 score:  -21.0  average score -20.7 best score -20.65 epsilon 0.81 steps 18806\n",
      "episode:  21 score:  -21.0  average score -20.7 best score -20.65 epsilon 0.80 steps 19868\n",
      "episode:  22 score:  -20.0  average score -20.7 best score -20.65 epsilon 0.79 steps 20910\n",
      "episode:  23 score:  -19.0  average score -20.6 best score -20.65 epsilon 0.78 steps 21996\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  24 score:  -21.0  average score -20.6 best score -20.58 epsilon 0.77 steps 22884\n",
      "episode:  25 score:  -21.0  average score -20.6 best score -20.58 epsilon 0.76 steps 23736\n",
      "episode:  26 score:  -21.0  average score -20.6 best score -20.58 epsilon 0.75 steps 24691\n",
      "episode:  27 score:  -20.0  average score -20.6 best score -20.58 epsilon 0.74 steps 25580\n",
      "episode:  28 score:  -20.0  average score -20.6 best score -20.58 epsilon 0.73 steps 26542\n",
      "episode:  29 score:  -20.0  average score -20.6 best score -20.58 epsilon 0.72 steps 27681\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  30 score:  -19.0  average score -20.5 best score -20.57 epsilon 0.71 steps 28892\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  31 score:  -17.0  average score -20.4 best score -20.52 epsilon 0.70 steps 30239\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  32 score:  -19.0  average score -20.4 best score -20.41 epsilon 0.69 steps 31271\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  33 score:  -21.0  average score -20.4 best score -20.36 epsilon 0.68 steps 32231\n",
      "episode:  34 score:  -19.0  average score -20.3 best score -20.36 epsilon 0.67 steps 33435\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  35 score:  -17.0  average score -20.2 best score -20.34 epsilon 0.65 steps 35134\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  36 score:  -17.0  average score -20.2 best score -20.25 epsilon 0.64 steps 36454\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  37 score:  -18.0  average score -20.1 best score -20.16 epsilon 0.62 steps 37907\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  38 score:  -20.0  average score -20.1 best score -20.11 epsilon 0.61 steps 39283\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  39 score:  -18.0  average score -20.1 best score -20.10 epsilon 0.59 steps 40658\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  40 score:  -20.0  average score -20.0 best score -20.05 epsilon 0.58 steps 41686\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  41 score:  -20.0  average score -20.0 best score -20.05 epsilon 0.57 steps 42886\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  42 score:  -19.0  average score -20.0 best score -20.05 epsilon 0.56 steps 43994\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  43 score:  -21.0  average score -20.0 best score -20.02 epsilon 0.55 steps 44919\n",
      "episode:  44 score:  -19.0  average score -20.0 best score -20.02 epsilon 0.54 steps 46331\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode:  45 score:  -19.0  average score -20.0 best score -20.02 epsilon 0.53 steps 47463\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-79f3675f2bcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m             agent.store_experience(observation, action,\n\u001b[1;32m     33\u001b[0m                                      reward, observation_, done)\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mn_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ec2f17a367ed>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_step_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/optim/rmsprop.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RMSprop does not support sparse gradients'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;31m# State initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mrelevant_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__hash__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/overrides.py\u001b[0m in \u001b[0;36mhas_torch_function\u001b[0;34m(relevant_args)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[0mimplementations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \"\"\"\n\u001b[0;32m-> 1083\u001b[0;31m     return _is_torch_function_enabled() and any(\n\u001b[0m\u001b[1;32m   1084\u001b[0m         \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__torch_function__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disabled_torch_function_impl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/overrides.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     return _is_torch_function_enabled() and any(\n\u001b[1;32m   1084\u001b[0m         \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__torch_function__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disabled_torch_function_impl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m         \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_disabled_torch_function_impl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = make_env('PongNoFrameskip-v4')\n",
    "best_score = -np.inf\n",
    "load_checkpoint = False\n",
    "n_games = 250\n",
    "\n",
    "agent = DQNAgent(gamma=0.99, epsilon=1, lr=0.0001, input_dims=(env.observation_space.shape), \n",
    "                 n_actions=env.action_space.n, mem_size=50000, eps_min=0.1,\n",
    "                 batch_size=32, replace=1000, eps_dec=1e-5,\n",
    "                 chkpt_dir='/Users/gregorywullaert/Documents/Reinforcement-Learning/DQN', algo='DQNAgent',\n",
    "                 env_name='PongNoFrameskip-v4')\n",
    "\n",
    "if load_checkpoint:\n",
    "    agent.load_models()\n",
    "\n",
    "fname = agent.algo + '_' + agent.env_name + '_lr' + str(agent.lr) + '_' + str(n_games) + 'games'\n",
    "figure_file = 'plots/' + fname + '.png'\n",
    "\n",
    "n_steps = 0\n",
    "scores, eps_history, steps_array = [], [], []\n",
    "\n",
    "for i in range(n_games):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "\n",
    "        if not load_checkpoint:\n",
    "            agent.store_experience(observation, action,\n",
    "                                     reward, observation_, done)\n",
    "            agent.learn()\n",
    "        observation = observation_\n",
    "        n_steps += 1\n",
    "    scores.append(score)\n",
    "    steps_array.append(n_steps)\n",
    "\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    print('episode: ', i,'score: ', score,\n",
    "             ' average score %.1f' % avg_score, 'best score %.2f' % best_score,\n",
    "            'epsilon %.2f' % agent.epsilon, 'steps', n_steps)\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        if not load_checkpoint:\n",
    "            agent.save_models()\n",
    "        best_score = avg_score\n",
    "\n",
    "    eps_history.append(agent.epsilon)\n",
    "\n",
    "x = [i+1 for i in range(len(scores))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
