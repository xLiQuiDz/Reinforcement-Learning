{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-level Control through Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch as T\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import collections \n",
    "import cv2\n",
    "\n",
    "from ipynb.fs.full.Plotting import plot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of Images\n",
    "\n",
    "Preprocessing the modules is optional but speeds up the performance of the agent. The following bullets summarise what needs to be done in order to preprocess the images. \n",
    "- Go from three channels to one channel\n",
    "    - Screen images have three channels, while our agent only needs one channel. We convert the image to grayscale.\n",
    "- Downscale to 84 x 84\n",
    "    - Images are realy large, with makes training slow. I am resizing the image to 84 x 84 to improve learning.\n",
    "- Take max of previous two frames\n",
    "    - We keep track of the two most recent frames and taking the max over the two. This is needed to overcome flickering in some Atari games.  \n",
    "- Repeat each action for four steps\n",
    "    - We repeat the same action four times for every skipped frame. This allows us to play n times more.\n",
    "- Swap channels to first possition\n",
    "    - Pytorch expects that images have channels first, while the OpenAI Gym returns images with channels last. We fix this by swapping the axis of the NumPy array. \n",
    "- Scale output\n",
    "    - Scale output, since they are integers from 0 to 255. We can deal with this by dividing the image by 255. \n",
    "- Stack four most recent frames\n",
    "\n",
    "References:\n",
    "https://github.com/openai/gym/tree/master/gym/wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derives from: Gym.Wrapper.\n",
    "# Repeating actions over k'th skipped frames and taking max off the previous two frames to overcome flikkering. \n",
    "\n",
    "class RepeatAction(gym.Wrapper):\n",
    "    def __init__(self, env, repeat, clip_reward):\n",
    "        super(RepeatAction, self).__init__(env)\n",
    "        \n",
    "        self.repeat = repeat\n",
    "        self.clip_reward = clip_reward\n",
    "        \n",
    "        self.shape = env.observation_space.low.shape\n",
    "        self.frame_buffer = np.zeros_like((2, self.shape))\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        score = 0.0\n",
    "\n",
    "        # Repeat actions in the environment.\n",
    "        for i in range(self.repeat):\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            \n",
    "            # Clip reward.\n",
    "            if self.clip_reward:\n",
    "                reward = np.clip(np.array([reward]), -1, 1)[0]  \n",
    "            score += reward\n",
    "            self.frame_buffer[i % 2] = observation\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1]) # Return max frame.\n",
    "        \n",
    "        return max_frame, score, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.frame_buffer = np.zeros_like((2,self.shape))\n",
    "        self.frame_buffer[0] = observation\n",
    "\n",
    "        return observation\n",
    "\n",
    "# Derives from: Gym.ObservationWrapper.\n",
    "# Reshape frame and convert to grayscale.\n",
    "\n",
    "class PreprocessFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super(PreprocessFrame, self).__init__(env)\n",
    "        \n",
    "        self.shape = (shape[2], shape[0], shape[1]) # PyTorch expects image channels first.\n",
    "        self.observation_space = gym.spaces.Box(low = 0.0, high = 1.0, shape = self.shape, dtype = np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        gray_frame = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY) # Convert to grayscale.\n",
    "        resized_frame = cv2.resize(gray_frame, self.shape[1:], interpolation = cv2.INTER_AREA) # Resize frame.\n",
    "        new_observation = np.array(resized_frame, dtype = np.uint8).reshape(self.shape) # Convert type.\n",
    "        new_observation = new_observation / 255.0 # RGB ranges from 0 to 255, observation space ranges from 0 to 1.\n",
    "\n",
    "        return new_observation\n",
    "\n",
    "# Derives from: Gym.ObservationWrapper.\n",
    "# Stack frames.\n",
    "\n",
    "class StackFrames(gym.ObservationWrapper):\n",
    "    def __init__(self, env, repeat):\n",
    "        super(StackFrames, self).__init__(env)\n",
    "        \n",
    "        self.stack = collections.deque(maxlen = repeat)\n",
    "        self.observation_space = gym.spaces.Box(env.observation_space.low.repeat(repeat, axis = 0),\n",
    "                                                env.observation_space.high.repeat(repeat, axis = 0),\n",
    "                                                dtype = np.float32)\n",
    "        \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.stack.clear()\n",
    "\n",
    "        for _ in range(self.stack.maxlen):\n",
    "            self.stack.append(observation)\n",
    "\n",
    "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.stack.append(observation)\n",
    "\n",
    "        return np.array(self.stack).reshape(self.observation_space.low.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, shape = (84, 84, 1), repeat = 4, clip_rewards = False):\n",
    "    env = gym.make(env_name)\n",
    "    env = RepeatAction(env, repeat, clip_rewards)\n",
    "    env = PreprocessFrame(env, shape)\n",
    "    env = StackFrames(env, repeat)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, observation_space, capacity):\n",
    "        \n",
    "        self.observation_space = observation_space \n",
    "        self.capacity = capacity\n",
    "        self.replay_counter = 0\n",
    "        \n",
    "        self.observation_memory = np.zeros((self.capacity, *self.observation_space), dtype = np.float32)\n",
    "        self.action_memory = np.zeros(self.capacity, dtype = np.int64)\n",
    "        self.reward_memory = np.zeros(self.capacity, dtype = np.float32)\n",
    "        self.next_observation_memory = np.zeros((self.capacity, *self.observation_space), dtype = np.float32)\n",
    "        self.terminal_memory = np.zeros(self.capacity, dtype = np.bool)\n",
    "\n",
    "    def store_experience(self, observation, action, reward, next_observation, done):\n",
    "        index = self.replay_counter % self.capacity\n",
    "        self.observation_memory[index] = observation\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.next_observation_memory[index] = next_observation\n",
    "        self.terminal_memory[index] = done\n",
    "        self.replay_counter += 1\n",
    "\n",
    "    def sample_memory(self, batch_size):\n",
    "        max_mem = min(self.replay_counter, self.capacity)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace = False)\n",
    "\n",
    "        observations = self.observation_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        next_observations = self.next_observation_memory[batch]\n",
    "        terminals = self.terminal_memory[batch]\n",
    "\n",
    "        return observations, actions, rewards, next_observations, terminals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, observation_space, action_space, learning_rate):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.observation_space[0], 32, 8, stride = 4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride = 2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, stride = 1)\n",
    "\n",
    "        self.fc_observation_space = self.calculate_convolutional_output(self.observation_space)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.fc_observation_space, 512)\n",
    "        self.fc2 = nn.Linear(512, self.action_space)\n",
    "\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr = self.learning_rate)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.to(self.device)\n",
    "\n",
    "    def calculate_convolutional_output(self, observation_space):\n",
    "        state = T.zeros(1, *observation_space)\n",
    "        dims = self.conv1(state)\n",
    "        dims = self.conv2(dims)\n",
    "        dims = self.conv3(dims)\n",
    "        return int(np.prod(dims.size()))\n",
    "\n",
    "    def forward(self, observation):\n",
    "        conv1 = F.relu(self.conv1(observation))\n",
    "        conv2 = F.relu(self.conv2(conv1))\n",
    "        conv3 = F.relu(self.conv3(conv2))\n",
    "        conv_state = conv3.view(conv3.size()[0], -1) # conv3 shape is BS x n_filters x H x W\n",
    "        flat1 = F.relu(self.fc1(conv_state)) # conv_state shape is BS x (n_filters * H * W)\n",
    "        actions = self.fc2(flat1)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, observation_space, action_space, learning_rate = 0.001, discount_rate = 0.99, exploration_rate = 1.0,\n",
    "                 max_exploration_rate = 1, min_exploration_rate = 0.01, exploration_decay_rate = 0.02,\n",
    "                 batch_size = 32, replace = 1000, capacity = 50000):\n",
    "        \n",
    "        self.episode = 0\n",
    "        self.learn_counter = 0 \n",
    "        \n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        \n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.max_exploration_rate = max_exploration_rate\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.replace = replace\n",
    "        self.capacity = capacity\n",
    "        \n",
    "        self.replay_memory = ReplayMemory(self.observation_space, self.capacity)\n",
    "        self.policy_network = DeepQNetwork(self.observation_space, self.action_space, self.learning_rate)\n",
    "        self.target_network = DeepQNetwork(self.observation_space, self.action_space, self.learning_rate)\n",
    "        \n",
    "    def choice_action(self, observation):\n",
    "        exploration_rate_threshold = np.random.random()\n",
    "        if exploration_rate_threshold > self.exploration_rate:\n",
    "            observation = T.tensor([observation], dtype = T.float).to(self.policy_network.device)\n",
    "            actions = self.policy_network.forward(observation) # Q-values for action in state.\n",
    "            action = T.argmax(actions).item() # Best Q-value for action in state.\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space) # Random Action.\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def store_experience(self, observation, action, reward, next_observation, done):\n",
    "        self.replay_memory.store_experience(observation, action, reward, next_observation, done)\n",
    "\n",
    "    def sample_memory(self):\n",
    "        observations, actions, rewards, next_observations, terminals = self.replay_memory.sample_memory(self.batch_size)\n",
    "        \n",
    "        return observations, actions, rewards, next_observations, terminals\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        if self.learn_counter % self.replace == 0:\n",
    "            self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "            \n",
    "    def decrease_exploration_rate(self):\n",
    "        self.exploration_rate = self.min_exploration_rate + \\\n",
    "            (self.max_exploration_rate - self.min_exploration_rate) * np.exp(- self.exploration_decay_rate * self.episode)\n",
    "        self.episode += 1\n",
    "\n",
    "    def learn(self):\n",
    "        if self.replay_memory.replay_counter < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Increment learn counter.\n",
    "        self.learn_counter += 1\n",
    "        \n",
    "        # Set gradient to zero.\n",
    "        self.policy_network.optimizer.zero_grad()\n",
    "        \n",
    "        # Copy policy network to target network after.\n",
    "        self.replace_target_network()\n",
    "\n",
    "        # Sample from replay memory.\n",
    "        observations, actions, rewards, next_observations, terminals = self.sample_memory()\n",
    "        \n",
    "        # Convert to tensors.\n",
    "        observations = T.tensor(observations).to(self.policy_network.device)\n",
    "        actions = T.tensor(actions).to(self.policy_network.device)\n",
    "        rewards = T.tensor(rewards).to(self.policy_network.device)\n",
    "        next_observations = T.tensor(next_observations).to(self.policy_network.device)\n",
    "        terminals = T.tensor(terminals).to(self.policy_network.device)\n",
    "        \n",
    "        indices = np.arange(self.batch_size)\n",
    "\n",
    "        # Calculate loss using Bellman equation.\n",
    "        q_pred = self.policy_network.forward(observations)[indices, actions]\n",
    "        q_next = self.target_network.forward(next_observations).max(dim = 1)[0]\n",
    "        q_next[terminals] = 0.0\n",
    "        q_target = rewards + self.discount_rate * q_next\n",
    "        loss = self.policy_network.loss(q_target, q_pred).to(self.policy_network.device)\n",
    "        \n",
    "        # Optimizer in action.\n",
    "        loss.backward()\n",
    "        self.policy_network.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10\n",
    "env = make_env('PongNoFrameskip-v4')\n",
    "agent = Agent(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "scores, exploration_rate_history = [], []\n",
    "\n",
    "for i in range(episodes):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    score = 0.0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choice_action(observation)\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "        agent.store_experience(observation, action, reward, next_observation, done)\n",
    "        agent.learn()\n",
    "        observation = next_observation\n",
    "        score += reward\n",
    "    \n",
    "    scores.append(score)\n",
    "    exploration_rate_history.append(agent.exploration_rate)\n",
    "    agent.decrease_exploration_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-21.0, -20.0, -20.0, -21.0, -21.0, -21.0, -19.0, -21.0, -19.0, -21.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 0.9803966865736877,\n",
       " 0.9611815447608,\n",
       " 0.9423468882484062,\n",
       " 0.9238851829227694,\n",
       " 0.9057890438555999,\n",
       " 0.888051232349986,\n",
       " 0.8706646530448178,\n",
       " 0.8536223510765493]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploration_rate_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
