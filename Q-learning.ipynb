{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most well-known algorithms used in reinforcement learning is Q-learning. Q-learning is an RL technique used for learning the optimal policy in a Markov decision process. Q-learning strives to find the optimal policy by learning the optimal Q-value in each state-action pair. The algorithm iteratively updates the Q-values in its Q-table for every state-action pair using the Bellman optimality equation. It does so until the Q-values nearly converges to the optimal Q-function. Initially, the Q-table for every state-action pair is initialized to zero. This is because the agent does not know anything about the environment at the start of an episode. We define a sequence of states, actions, and rewards, which ends in a terminal condition as an episode. The Q-values will be iteratively updated during various episodes using this value iteration method. After each episode, the Q-table will slowly converge to the optimal Q-value. The agent can make better decisions during episodes in every state based on the Q-table's highest values for that state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, observation_space, action_space, learning_rate = 0.1, discount_rate = 0.99, exploration_rate = 1.0, \n",
    "                 max_exploration_rate = 1, min_exploration_rate = 0.01, exploration_decay_rate = 0.001):\n",
    "        \n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.max_exploration_rate = max_exploration_rate\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate\n",
    "        \n",
    "        self.episode = 0\n",
    "        self.Q = np.zeros((self.observation_space, self.action_space))\n",
    "        \n",
    "    def choice_action(self, state):\n",
    "        exploration_rate_threshold = np.random.random()\n",
    "        if exploration_rate_threshold > self.exploration_rate:\n",
    "            action = np.argmax(self.Q[state,:])\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "            \n",
    "        return action\n",
    "        \n",
    "    def decrease_exploration_rate(self):\n",
    "        self.episode += 1\n",
    "        self.exploration_rate = self.min_exploration_rate + \\\n",
    "            (self.max_exploration_rate - self.min_exploration_rate) * np.exp(- self.exploration_decay_rate * self.episode)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        self.Q[state, action] = self.Q[state, action] * (1 - self.learning_rate) + \\\n",
    "            self.learning_rate * (reward + self.discount_rate * np.max(self.Q[next_state, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.04200000000000003\n",
      "2000 :  0.18900000000000014\n",
      "3000 :  0.3950000000000003\n",
      "4000 :  0.5530000000000004\n",
      "5000 :  0.6190000000000004\n",
      "6000 :  0.6750000000000005\n",
      "7000 :  0.6720000000000005\n",
      "8000 :  0.6770000000000005\n",
      "9000 :  0.7100000000000005\n",
      "10000 :  0.6960000000000005\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "agent = Agent(env.observation_space.n,  env.action_space.n)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choice_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.learn(state, action, reward, next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        score += reward\n",
    "\n",
    "    scores.append(score)\n",
    "    agent.decrease_exploration_rate()\n",
    "\n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(scores), episodes / 1000)\n",
    "count = 1000\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    avg_reward = sum(r/1000)\n",
    "    print(count, \": \", str(avg_reward))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
