{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning with Double Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "- https://gym.openai.com/envs/Pong-v0/\n",
    "- https://github.com/openai/gym/tree/master/gym/wrappers\n",
    "- https://github.com/philtabor/Deep-Q-Learning-Paper-To-Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch as T\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import collections \n",
    "import cv2\n",
    "\n",
    "from ipynb.fs.full.Plotting import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derives from: Gym.Wrapper.\n",
    "# Repeating actions over k'th skipped frames and taking max off the previous two frames to overcome flikkering. \n",
    "\n",
    "class RepeatAction(gym.Wrapper):\n",
    "    def __init__(self, env, repeat, clip_reward):\n",
    "        super(RepeatAction, self).__init__(env)\n",
    "        \n",
    "        self.repeat = repeat\n",
    "        self.clip_reward = clip_reward\n",
    "        \n",
    "        self.shape = env.observation_space.low.shape\n",
    "        self.frame_buffer = np.zeros_like((2, self.shape))\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        score = 0.0\n",
    "\n",
    "        # Repeat actions in the environment.\n",
    "        for i in range(self.repeat):\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            \n",
    "            # Clip reward.\n",
    "            if self.clip_reward:\n",
    "                reward = np.clip(np.array([reward]), -1, 1)[0]  \n",
    "            score += reward\n",
    "            self.frame_buffer[i % 2] = observation\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1]) # Return max frame.\n",
    "        \n",
    "        return max_frame, score, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.frame_buffer = np.zeros_like((2,self.shape))\n",
    "        self.frame_buffer[0] = observation\n",
    "\n",
    "        return observation\n",
    "\n",
    "# Derives from: Gym.ObservationWrapper.\n",
    "# Reshape frame and convert to grayscale.\n",
    "\n",
    "class PreprocessFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super(PreprocessFrame, self).__init__(env)\n",
    "        \n",
    "        self.shape = (shape[2], shape[0], shape[1]) # PyTorch expects image channels first.\n",
    "        self.observation_space = gym.spaces.Box(low = 0.0, high = 1.0, shape = self.shape, dtype = np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        gray_frame = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY) # Convert to grayscale.\n",
    "        resized_frame = cv2.resize(gray_frame, self.shape[1:], interpolation = cv2.INTER_AREA) # Resize frame.\n",
    "        new_observation = np.array(resized_frame, dtype = np.uint8).reshape(self.shape) # Convert type.\n",
    "        new_observation = new_observation / 255.0 # RGB ranges from 0 to 255, observation space ranges from 0 to 1.\n",
    "\n",
    "        return new_observation\n",
    "\n",
    "# Derives from: Gym.ObservationWrapper.\n",
    "# Stack frames.\n",
    "\n",
    "class StackFrames(gym.ObservationWrapper):\n",
    "    def __init__(self, env, repeat):\n",
    "        super(StackFrames, self).__init__(env)\n",
    "        \n",
    "        self.stack = collections.deque(maxlen = repeat)\n",
    "        self.observation_space = gym.spaces.Box(env.observation_space.low.repeat(repeat, axis = 0),\n",
    "                                                env.observation_space.high.repeat(repeat, axis = 0),\n",
    "                                                dtype = np.float32)\n",
    "        \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.stack.clear()\n",
    "\n",
    "        for _ in range(self.stack.maxlen):\n",
    "            self.stack.append(observation)\n",
    "\n",
    "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.stack.append(observation)\n",
    "\n",
    "        return np.array(self.stack).reshape(self.observation_space.low.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, shape = (84, 84, 1), repeat = 4, clip_rewards = False):\n",
    "    env = gym.make(env_name)\n",
    "    env = RepeatAction(env, repeat, clip_rewards)\n",
    "    env = PreprocessFrame(env, shape)\n",
    "    env = StackFrames(env, repeat)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, observation_space, capacity):\n",
    "        \n",
    "        self.observation_space = observation_space \n",
    "        self.capacity = capacity\n",
    "        self.replay_counter = 0\n",
    "        \n",
    "        self.observation_memory = np.zeros((self.capacity, *self.observation_space), dtype = np.float32)\n",
    "        self.action_memory = np.zeros(self.capacity, dtype = np.int64)\n",
    "        self.reward_memory = np.zeros(self.capacity, dtype = np.float32)\n",
    "        self.next_observation_memory = np.zeros((self.capacity, *self.observation_space), dtype = np.float32)\n",
    "        self.terminal_memory = np.zeros(self.capacity, dtype = np.bool)\n",
    "\n",
    "    def store_experience(self, observation, action, reward, next_observation, done):\n",
    "        index = self.replay_counter % self.capacity\n",
    "        self.observation_memory[index] = observation\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.next_observation_memory[index] = next_observation\n",
    "        self.terminal_memory[index] = done\n",
    "        self.replay_counter += 1\n",
    "\n",
    "    def sample_memory(self, batch_size):\n",
    "        max_mem = min(self.replay_counter, self.capacity)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace = False)\n",
    "\n",
    "        observations = self.observation_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        next_observations = self.next_observation_memory[batch]\n",
    "        terminals = self.terminal_memory[batch]\n",
    "\n",
    "        return observations, actions, rewards, next_observations, terminals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDeepQNetwork(nn.Module):\n",
    "    def __init__(self, observation_space, action_space, learning_rate):\n",
    "        super(DoubleDeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.observation_space[0], 32, 8, stride = 4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride = 2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, stride = 1)\n",
    "\n",
    "        self.fc_observation_space = self.calculate_convolutional_output(self.observation_space)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.fc_observation_space, 512)\n",
    "        self.fc2 = nn.Linear(512, self.action_space)\n",
    "\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr = self.learning_rate)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.to(self.device)\n",
    "\n",
    "    def calculate_convolutional_output(self, observation_space):\n",
    "        state = T.zeros(1, *observation_space)\n",
    "        dims = self.conv1(state)\n",
    "        dims = self.conv2(dims)\n",
    "        dims = self.conv3(dims)\n",
    "        return int(np.prod(dims.size()))\n",
    "\n",
    "    def forward(self, observation):\n",
    "        conv1 = F.relu(self.conv1(observation))\n",
    "        conv2 = F.relu(self.conv2(conv1))\n",
    "        conv3 = F.relu(self.conv3(conv2))\n",
    "        conv_state = conv3.view(conv3.size()[0], -1) # Conv3 shape is BS x n_filters x H x W.\n",
    "        flat1 = F.relu(self.fc1(conv_state)) # Conv_state shape is BS x (n_filters * H * W).\n",
    "        actions = self.fc2(flat1)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, observation_space, action_space, learning_rate = 0.0001, discount_rate = 0.99, exploration_rate = 1.0,\n",
    "                 max_exploration_rate = 1, min_exploration_rate = 0.1, exploration_decay_rate = 0.01,\n",
    "                 batch_size = 32, replace = 1000, capacity = 50000):\n",
    "        \n",
    "        self.episode = 0\n",
    "        self.learn_counter = 0 \n",
    "        \n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        \n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.max_exploration_rate = max_exploration_rate\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.replace = replace\n",
    "        self.capacity = capacity\n",
    "        \n",
    "        self.replay_memory = ReplayMemory(self.observation_space, self.capacity)\n",
    "        self.policy_network = DoubleDeepQNetwork(self.observation_space, self.action_space, self.learning_rate)\n",
    "        self.target_network = DoubleDeepQNetwork(self.observation_space, self.action_space, self.learning_rate)\n",
    "        \n",
    "    def choice_action(self, observation):\n",
    "        exploration_rate_threshold = np.random.random()\n",
    "        if exploration_rate_threshold > self.exploration_rate:\n",
    "            observation = T.tensor([observation], dtype = T.float).to(self.policy_network.device)\n",
    "            actions = self.policy_network.forward(observation) # Q-values for action in state.\n",
    "            action = T.argmax(actions).item() # Best Q-value for action in state.\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space) # Random Action.\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def store_experience(self, observation, action, reward, next_observation, done):\n",
    "        self.replay_memory.store_experience(observation, action, reward, next_observation, done)\n",
    "\n",
    "    def sample_memory(self):\n",
    "        observations, actions, rewards, next_observations, terminals = self.replay_memory.sample_memory(self.batch_size)\n",
    "        \n",
    "        return observations, actions, rewards, next_observations, terminals\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        if self.learn_counter % self.replace == 0:\n",
    "            self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "            \n",
    "    def decrease_exploration_rate(self):\n",
    "        self.exploration_rate = self.min_exploration_rate + \\\n",
    "            (self.max_exploration_rate - self.min_exploration_rate) * np.exp(- self.exploration_decay_rate * self.episode)\n",
    "        self.episode += 1\n",
    "\n",
    "    def learn(self):\n",
    "        if self.replay_memory.replay_counter < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Increment learn counter.\n",
    "        self.learn_counter += 1\n",
    "        \n",
    "        # Set gradient to zero.\n",
    "        self.policy_network.optimizer.zero_grad()\n",
    "        \n",
    "        # Copy policy network to target network after.\n",
    "        self.replace_target_network()\n",
    "\n",
    "        # Sample from replay memory.\n",
    "        observations, actions, rewards, next_observations, terminals = self.sample_memory()\n",
    "        \n",
    "        # Convert to tensors.\n",
    "        observations = T.tensor(observations).to(self.policy_network.device)\n",
    "        actions = T.tensor(actions).to(self.policy_network.device)\n",
    "        rewards = T.tensor(rewards).to(self.policy_network.device)\n",
    "        next_observations = T.tensor(next_observations).to(self.policy_network.device)\n",
    "        terminals = T.tensor(terminals).to(self.policy_network.device)\n",
    "        \n",
    "        indices = np.arange(self.batch_size)\n",
    "\n",
    "        # Calculate loss using Bellman equation.\n",
    "        q_pred = self.policy_network.forward(observations)[indices, actions]\n",
    "        q_next = self.target_network.forward(next_observations)\n",
    "        q_eval = self.policy_network.forward(next_observations)\n",
    "        max_actions = T.argmax(q_eval, dims = 1)\n",
    "        q_next[terminals] = 0.0\n",
    "        q_target = rewards + self.discount_rate * q_next[indices, max_actions]\n",
    "        loss = self.policy_network.loss(q_target, q_pred).to(self.policy_network.device)\n",
    "        \n",
    "        # Optimizer in action.\n",
    "        loss.backward()\n",
    "        self.policy_network.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-aa2e4e16169d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-df5b352414f5>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# Optimizer in action.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "episodes = 500\n",
    "\n",
    "env = make_env('PongNoFrameskip-v4')\n",
    "agent = Agent(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "scores, exploration_rate_history, steps = [], [], []\n",
    "\n",
    "for i in range(episodes):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    score = 0.0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choice_action(observation)\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "        agent.store_experience(observation, action, reward, next_observation, done)\n",
    "        agent.learn()\n",
    "        observation = next_observation\n",
    "        score += reward\n",
    "        step += 1\n",
    "    \n",
    "    scores.append(score)\n",
    "    steps.append(step)\n",
    "    exploration_rate_history.append(agent.exploration_rate)\n",
    "    agent.decrease_exploration_rate()\n",
    "\n",
    "plot_learning_curve(steps, scores, exploration_rate_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
