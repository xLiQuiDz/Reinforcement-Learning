{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-level Control through Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch as T\n",
    "import gym\n",
    "\n",
    "import collections\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of Images\n",
    "- Go from three channels to one channel\n",
    "    - Screen images have three channels, while our agent only needs one channel. We convert the image to grayscale.\n",
    "- Downscale to 84 x 84\n",
    "    - Images are realy large, with makes training slow. I am resizing the image to 84 x 84 to improve learning.\n",
    "- Take max of previous frames\n",
    "    - We keep track of the two most recent frames and taking the max over the two. \n",
    "- Repeat action 4 times\n",
    "    - We repeat the same action four times for every skipped frame.\n",
    "- Scale output\n",
    "    - Scale output, since they are integers from 0 to 255. We can deal with this by dividing the image by 255. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/openai/gym/tree/master/gym/wrappers\n",
    "# Class for repeating action and taking max frame over the previous two frames.\n",
    "\n",
    "class RepeatActionAndMaxFrame(gym.Wrapper):\n",
    "    def __init__(self, env = None, repeat = 4, clip_reward = False, no_ops = 0, fire_fist = False):\n",
    "        super(RepeatActionAndMaxFrame, self).__init__(env)\n",
    "        \n",
    "        self.repeat = repeat\n",
    "        self.shape = env.observation_space.low.shape\n",
    "        self.frame_buffer = np.zeros_like((2, self.shape))\n",
    "        self.clip_reward = clip_reward\n",
    "        self.no_ops = no_ops\n",
    "        self.fire_fist = fire_fist\n",
    "        \n",
    "    def step(self, action):\n",
    "        t_reward = 0.0\n",
    "        done = False\n",
    "        \n",
    "        for i in range(self.repeat):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            \n",
    "            if self.clip_reward:\n",
    "                reward = np.clip(np.array([reward]), -1, 1)[0]\n",
    "            \n",
    "            t_reward += reward\n",
    "            \n",
    "            idx = i % 2\n",
    "            self.frame_buffer[idx] = obs\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])\n",
    "        return max_frame, t_reward, done, info\n",
    "    \n",
    "    def rest(self):\n",
    "        obs = self.env.reset()\n",
    "        no_obs = np.randint(self.no_ops) + 1 if self.no_ops > 0 else 0\n",
    "        \n",
    "        for _ in range(no_ops):\n",
    "            _, _, done, _ = self.env.step(0)\n",
    "            if done:\n",
    "                self.env.reset()\n",
    "        \n",
    "        if self.fire_first:\n",
    "            assert self.env.unwrapper.get_action_meaning()[1] == 'FIRE'\n",
    "            obs, _, _, _ = self.env.step(1)\n",
    "        \n",
    "        self.frame_buffer = np.zeros_like((2, self.shape))\n",
    "        self.frame_buffer[0] = obs\n",
    "        \n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, shape, env = None):\n",
    "        super(preprocessFrame, self).__init__(env)\n",
    "        \n",
    "        self.shape = (shape[2], shape[0], shape[1]) # Order in with Pytorch accepts parameters.\n",
    "        self.observation_space = gym.spaces.Box(low = 0.0, high = 1.0, shape = self.shape, dtype = np.float32) # Scale frame.\n",
    "        \n",
    "    def observation(self, obs):\n",
    "        new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "        resized_screen = cv2.resize(new_frame, self.shape[1:], interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        #\n",
    "        new_obs = np.array(resized_screen, dtype = np.uint8).reshape(self.shape)\n",
    "        new_obs = new_obs / 255.0\n",
    "        \n",
    "        return new_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackFrames(gym.ObservationWrapper):\n",
    "    def __init__(self, env, repeat):\n",
    "        super(StackFrames, self).__init__(env)\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(env.observation_space.low.repeat(repeat, axis = 0),\n",
    "                                               env.observation_space.high.repeat(repeat, axis = 0),\n",
    "                                               dtype = np.float32)\n",
    "        self.stack = collection.deque(maxlen = repeat)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.stack.clear()\n",
    "        observation = self.env.reset()\n",
    "            \n",
    "        for _ in range(self.stack.maxlen):\n",
    "            self.stack.append(observation)\n",
    "                \n",
    "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        self.stack.append(observation)\n",
    "        \n",
    "        return np.array(self.stack).reshape(self.observation_space.low.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, shape = (84, 84, 1), repeat = 4, clip_rewards = False, \n",
    "             no_ops = 0, fire_first = False):\n",
    "    \n",
    "    gym.make(env_name)\n",
    "    env = RepeatActionAndMaxFrame(env, repeat, clip_rewards, no_ops, fire_first)\n",
    "    env = preprocessFrame(shape, env)\n",
    "    env = StackFrames(env, repeat)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
