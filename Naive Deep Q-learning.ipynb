{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements a deep Q-learning algorithm in combination with a neural network. The neural network's objective is to approximate the optimal Q-value for each state-action pair in our environment. \n",
    "\n",
    "We use a neural network with two linear layers that take a low-level representation of the environment as input and output the Q-values, corresponding to the actions an agent can take from that state. Adam will be used as an optimizer with mean square error as a loss function. The learning rate will be specified as 0.001. This naive deep Q-learning algorithm's overall goal is to serve as a benchmark for further improvement, and thus hyperparameter tuning will not be considered in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'test' from 'ipynb.fs.full.Plotting' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0fe8ac61a68a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mipynb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPlotting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'test' from 'ipynb.fs.full.Plotting' (unknown location)"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch as T\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from ipynb.fs.full.Plotting import test\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Deep Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDeepQNetwork(nn.Module):\n",
    "    def __init__(self, observation_space, action_space, learning_rate):\n",
    "        super(LinearDeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(*observation_space, 128)\n",
    "        self.fc2 = nn.Linear(128, action_space)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        layer1 = F.relu(self.fc1(state))\n",
    "        actions = self.fc2(layer1)\n",
    "        \n",
    "        return actions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, observation_space, action_space, learning_rate = 0.001, discount_rate = 0.99, exploration_rate = 1.0,\n",
    "                 max_exploration_rate = 1, min_exploration_rate = 0.01,  exploration_decay_rate = 0.001):\n",
    "        \n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        \n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.max_exploration_rate = max_exploration_rate\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate\n",
    "        \n",
    "        self.episode = 0\n",
    "        self.Q = LinearDeepQNetwork(self.observation_space, self.action_space, self.learning_rate)\n",
    "        \n",
    "    def choice_action(self, state):\n",
    "        exploration_rate_threshold = np.random.random()\n",
    "        if exploration_rate_threshold > self.exploration_rate:\n",
    "            state = T.tensor(state, dtype = T.float).to(self.Q.device)\n",
    "            actions = self.Q.forward(state) # Action of state.\n",
    "            action = T.argmax(actions).item() # Best action.\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def decrease_exploration_rate(self):\n",
    "        self.episode += 1\n",
    "        self.exploration_rate = self.min_exploration_rate + \\\n",
    "            (self.max_exploration_rate - self.min_exploration_rate) * np.exp(- self.exploration_decay_rate * self.episode)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        # Set gradient to zero.\n",
    "        self.Q.optimizer.zero_grad()\n",
    "        \n",
    "        # Convert to tensors.\n",
    "        states = T.tensor(state, dtype = T.float).to(self.Q.device)\n",
    "        actions = T.tensor(action).to(self.Q.device)\n",
    "        rewards = T.tensor(reward).to(self.Q.device)\n",
    "        next_states = T.tensor(next_state, dtype = T.float).to(self.Q.device)\n",
    "        \n",
    "        # Calculate loss using Bellman equation.\n",
    "        q_pred = self.Q.forward(states)[actions]\n",
    "        q_next = self.Q.forward(next_states).max()\n",
    "        q_target = reward + self.exploration_rate * q_next\n",
    "        loss = self.Q.loss(q_target, q_pred).to(self.Q.device)\n",
    "        \n",
    "        # Optimizer in action.\n",
    "        loss.backward()\n",
    "        self.Q.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0 \t Average score:  12.0 \t Exploration Rate:  0.9990104948350412\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a21c35ed6ff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Average score: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Exploration Rate: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploration_rate_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/Reinforcement Learning/Plotting.ipynb\u001b[0m in \u001b[0;36mplot\u001b[0;34m(episodes, scores, average_scores, exploration_rate_history)\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m    \"source\": [\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m\"# Plotting\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m    ]\n\u001b[1;32m      9\u001b[0m   },\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "env = gym.make('CartPole-v1')\n",
    "agent = Agent(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "scores = []\n",
    "average_scores = []\n",
    "exploration_rate_history = []\n",
    "\n",
    "\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choice_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.learn(state, action, reward, next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        score += reward\n",
    "\n",
    "    scores.append(score)\n",
    "    agent.decrease_exploration_rate()\n",
    "    \n",
    "    exploration_rate_history.append(agent.exploration_rate)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        average_scores.append(avg_score)\n",
    "        print(\"Episode: \", i, \"\\t\", \"Average score: \", avg_score, \"\\t\", \"Exploration Rate: \", agent.exploration_rate)\n",
    "\n",
    "plot(episodes, scores, average_scores, exploration_rate_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fbd55f77ab7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
